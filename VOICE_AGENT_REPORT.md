# Voice Agent: Post-Discharge Orchestration System

## Scope of Work

### Overview

The Voice Agent system represents a comprehensive multi-agent orchestration platform designed to address critical gaps in post-discharge patient care through automated, intelligent voice and text interactions. Hospital readmissions within 30 days of discharge remain a significant healthcare challenge, with studies indicating that up to 20% of Medicare patients are readmitted within this window, often due to inadequate monitoring, medication non-adherence, or delayed recognition of complications. The Voice Agent system was developed to mitigate these risks by providing continuous, accessible patient support that bridges the gap between hospital discharge and outpatient follow-up care.

Traditional post-discharge care relies heavily on scheduled phone calls from nursing staff, patient-initiated contact with overwhelmed clinical teams, and paper-based symptom tracking. These approaches suffer from scalability limitations, inconsistent application, and delayed intervention when patients experience concerning symptoms. The Voice Agent system addresses these limitations through a 24/7 conversational interface that combines natural language understanding, clinical decision support, and policy-aware scheduling to deliver personalized, guideline-compliant care at scale.

### Primary Objectives

The development of the Voice Agent system was guided by five core objectives that reflect both clinical imperatives and operational requirements in modern healthcare delivery. First, the system aims to reduce hospital readmissions by enabling early detection of concerning symptoms through continuous monitoring and intelligent triage. By implementing a three-tier classification system that directs patients to appropriate care levels based on symptom severity, the system seeks to intervene before conditions escalate to emergency situations requiring hospitalization.

Second, the system was designed to improve patient engagement by providing accessible, conversational interfaces for healthcare management. Rather than requiring patients to navigate complex phone systems or wait on hold for clinical staff, the Voice Agent offers immediate responses to common questions about appointments, medications, and symptoms. This accessibility is particularly important for elderly patients, those with limited health literacy, or individuals managing multiple chronic conditions who may struggle with traditional healthcare interfaces.

Third, the system addresses the critical issue of medication adherence, which remains a leading cause of preventable readmissions. By delivering on-demand medication education covering side effects, drug interactions, dosing instructions, and contraindications, the Voice Agent empowers patients to understand their medications and make informed decisions about their care. The system also provides guidance for common medication issues such as missed doses or confusion about administration, reducing the need for urgent care visits due to medication-related concerns.

Fourth, recognizing the vital role of family caregivers in post-discharge care, the system supports caregiver engagement through automated weekly summaries of patient status. These summaries aggregate symptom reports, medication adherence data, and appointment information into digestible formats that enable caregivers to monitor their loved ones' health trajectories and identify concerning trends. This capability is particularly valuable for patients with cognitive impairments or complex medical needs who require additional oversight.

Finally, the system was designed to enforce clinical policies and business rules that ensure appropriate, compliant care delivery. This includes validation of minor consent requirements, specialist referral protocols, telehealth eligibility criteria, and post-operative monitoring windows. By encoding these policies in executable logic, the system ensures consistent application across all patient interactions while reducing administrative burden on clinical staff.

### Scope Boundaries

The current implementation of the Voice Agent system focuses on a well-defined set of capabilities that demonstrate feasibility and clinical utility while maintaining realistic boundaries for a capstone project. The system handles intent detection and routing for patient queries across five specialized domains: appointment management, symptom monitoring, medication education, caregiver support, and general assistance. For appointment management, the system supports scheduling, rescheduling, cancellation, and status checking with integrated triage logic that escalates emergency symptoms appropriately. Symptom monitoring capabilities include logging patient-reported symptoms with 0-10 severity scaling, tracking fever temperatures, and applying triage rules to classify urgency.

The medication education component addresses six common query categories: side effects, missed doses, drug interactions, administration instructions, contraindications, and general medication information. Caregiver support functionality generates weekly summaries with proper consent enforcement and privacy safeguards. Throughout all interactions, the system enforces business policies including minor consent validation, specialist referral requirements, telehealth eligibility assessment, and post-operative monitoring windows. The system supports both speech-to-text and text-to-speech interactions, enabling truly conversational experiences, and maintains comprehensive logging infrastructure for audit trails and continuous improvement.

Several important capabilities were explicitly excluded from the current scope to maintain project feasibility and focus. The system does not integrate directly with electronic health record systems, instead utilizing a CSV-based local database that simulates realistic patient data structures. Real-time physician notifications are simulated through log entries rather than implemented as active alerting mechanisms. The system does not handle payment processing, insurance verification, or prescription fulfillment. Video telehealth session hosting and real-time vital sign monitoring from wearable devices are beyond the current scope, though architectural considerations allow for future expansion in these directions.

### Success Criteria

The Voice Agent system's effectiveness is evaluated against quantitative performance metrics and qualitative user experience criteria that reflect both technical accuracy and clinical safety imperatives. For intent classification, the system must achieve at least 95% accuracy in routing queries to the correct specialized agent, as misrouting could result in missed emergencies or inappropriate responses. Triage performance is held to an even higher standard: the system must achieve 100% sensitivity for RED flag symptoms, meaning it must never fail to escalate a true emergency. While false positives in emergency detection are clinically acceptable and err on the side of caution, false negatives represent unacceptable safety risks.

For medication risk assessment, the system must correctly assign risk tiers in at least 90% of queries, with particular emphasis on identifying high-risk scenarios such as double dosing or serious drug interactions. Policy compliance is non-negotiable: the system must enforce business rules for minor consent, referrals, and eligibility with 100% consistency to ensure regulatory compliance and appropriate care delivery. From a performance perspective, the system should maintain average response times under 2 seconds for operations not requiring large language model calls, ensuring that user experience remains fluid and conversation-like despite the computational complexity underlying responses.

Beyond these quantitative metrics, the system's conversational quality is assessed through natural language generation that demonstrates clarity, empathy, and actionability. Responses must be comprehensible to patients with varying health literacy levels, show appropriate concern for symptoms being reported, address user questions completely, and provide clear next steps that patients can follow. These qualitative criteria reflect the reality that even technically perfect systems will fail if patients find them difficult to use or trust.

---

## System Architecture

### High-Level Architecture

The Voice Agent system employs a graph-based multi-agent architecture implemented using LangGraph, a framework specifically designed for building stateful, multi-actor applications with large language models. This architectural choice reflects several key design principles: modularity, allowing individual agents to be developed and tested independently; flexibility, enabling dynamic routing based on conversation context; and maintainability, providing clear separation of concerns that facilitates future enhancements. The graph-based approach contrasts with traditional pipeline architectures by allowing conditional branching, parallel execution where appropriate, and explicit state management across conversation turns.

At the highest level, the system architecture comprises six major components connected through a directed graph structure. User inputs enter through a unified interface layer that abstracts away the modality of communication, whether text-based through a command-line interface, visual through a Streamlit web application, or voice-based through speech-to-text processing. This input is immediately passed to the routing agent node, which serves as the system's central intelligence for intent classification and patient identification. The routing agent analyzes the natural language input using a combination of large language model inference and rule-based fallback mechanisms to determine which specialized agent should handle the query.

Based on the routing decision, execution proceeds to one of five specialized agent nodes, each encapsulating domain-specific logic and knowledge. The appointment agent handles all scheduling operations while applying comprehensive triage logic and policy enforcement. The followup agent specializes in symptom logging and severity assessment, maintaining temporal records that enable trend analysis. The medication agent draws on a curated drug knowledge base to answer patient questions with appropriate risk warnings. The caregiver agent aggregates patient data to generate privacy-compliant weekly summaries for family members. The help agent serves as a catch-all for general conversation, greetings, and queries that fall outside other specialized domains.

All agents operate on a shared state structure that flows through the graph, accumulating information and responses as execution progresses. This state includes not only the user's current input and the agents' responses, but also conversation context such as patient identifiers, parsed entities, triage classifications, and session metadata. After agent execution completes, responses are aggregated, formatted appropriately for the output modality, and delivered to the user. If voice output is enabled, responses are synthesized using text-to-speech. All interactions are logged to structured JSONL files that maintain comprehensive audit trails while supporting analytics and continuous improvement efforts.

### State Management and Data Flow

The cornerstone of the system's architecture is a typed state dictionary that maintains conversation context as it flows through the graph. This state structure, formally defined as VoiceAgentState, contains fields for raw user input, extracted patient identifiers, classified intent, parsed structured data, individual agent responses, the final consolidated response, voice preferences, session tracking information, timestamps, and structured log entries. The use of a typed dictionary provides compile-time validation of state structure while maintaining the flexibility to extend the schema as new capabilities are added.

State flows through the graph in a unidirectional manner, with each node receiving the current state as input, performing its specialized processing, updating relevant state fields, and returning the modified state to the graph coordinator. This functional programming approach, where nodes are essentially pure functions of state, enables several important properties. First, it allows individual nodes to be tested in isolation by constructing appropriate input states and asserting expected output states. Second, it facilitates debugging by enabling inspection of state at any point in the execution graph. Third, it permits parallel execution of independent nodes when the graph structure allows, though the current implementation primarily uses sequential execution for simplicity.

The workflow orchestration layer, implemented in the workflow.py module, defines the graph structure by registering nodes, specifying entry points, and configuring conditional routing logic. When a user query arrives, the workflow begins execution at the routing node, which examines the input and updates the state with classified intent. A routing function then evaluates this intent field to determine the next node to execute. This conditional branching is expressed declaratively through the LangGraph API, which maps intent values to target node names. After the specialized agent completes its work, execution returns to the final aggregation step, which consolidates agent-specific responses into the primary response field that will be delivered to the user.

### Database Service Layer

Patient data access is abstracted through a database service layer that currently operates on CSV files but is designed to be replaceable with actual electronic health record integrations in production deployments. This abstraction layer, implemented in database.py, exposes methods for querying patient demographics, retrieving scheduled appointments, accessing medication prescriptions, looking up caregiver relationships, appending to symptom logs, and loading clinical knowledge bases. By centralizing all data access through this service, the system maintains a clean separation between business logic in the agents and data persistence mechanisms.

The CSV-based implementation stores patient information across several related files that simulate the structure of a realistic healthcare database. The patients file contains demographic information including names, dates of birth, contact details, and consent flags that control privacy-sensitive features like caregiver summaries. The appointments file records scheduled visits with fields for appointment identifiers, patient identifiers, dates, times, providers, specialties, and status flags indicating whether appointments are confirmed, completed, or cancelled. The prescriptions file maintains current medications for each patient, including drug names, dosages, frequencies, conditions being treated, and prescribing providers.

Additional knowledge bases support the agents' decision-making capabilities. The drug_knowledge file contains medication information sourced from publicly available drug databases, including common and serious side effects, missed dose guidance, food and timing instructions, known drug interactions, and contraindication warnings. The symptom_codes file maps colloquial symptom descriptions to standardized SNOMED CT codes, enabling structured symptom tracking that could integrate with external clinical systems. The policy_config file encodes business rules and triage thresholds in JSON format, allowing clinical stakeholders to adjust system behavior without modifying code.

### Large Language Model Integration

The system's natural language understanding and generation capabilities are powered by integration with large language model APIs, abstracted through a provider layer that supports multiple model families. This abstraction, implemented in utils/llm_provider.py, presents a unified interface for chat completion requests while handling provider-specific authentication, message formatting, error handling, and response parsing. The system currently supports OpenAI's GPT models, Anthropic's Claude models, and Google's Gemini models, selected through environment configuration without requiring code changes.

The choice to support multiple providers reflects both practical and architectural considerations. From a practical standpoint, different organizations may have established relationships with specific providers, existing security approvals, or cost constraints that favor particular models. Architecturally, the abstraction layer insulates the rest of the system from provider-specific APIs and data formats, reducing technical debt and enabling rapid switching if a particular provider experiences outages or deprecates models. The abstraction also facilitates A/B testing different models for specific tasks, allowing empirical comparison of accuracy, latency, and cost trade-offs.

Large language models are employed at several critical points in the system's processing pipeline. The routing agent uses chat completion to classify user intent from natural language queries, requesting structured JSON responses that can be reliably parsed. The appointment and followup agents employ models to extract structured information from free-text symptom descriptions, including symptom names, severity scores, body locations, and contextual details. The medication agent uses language models to parse drug names from colloquial references and classify query intent. The caregiver agent leverages generative capabilities to produce narrative summaries from structured data aggregations. Finally, the help agent employs open-ended conversation capabilities to handle general questions and maintain engaging interactions.

Model selection and prompting strategies were refined through iterative experimentation to balance accuracy, latency, and cost. For tasks requiring structured output like intent classification, prompts explicitly request JSON formatting and specify exact field names and value types. System messages establish role context, instructing the model that it is a healthcare assistant bound by safety and privacy requirements. Temperature parameters are set to zero for deterministic classification tasks and slightly elevated for generative tasks where some variability in phrasing improves naturalness. Token limits are configured conservatively to control costs while ensuring sufficient context for multi-turn conversations.

---

## Methodology

### Agent Design Philosophy

The development of each specialized agent followed a consistent methodology that prioritized clinical safety, policy compliance, and user experience while maintaining engineering best practices for testing and maintainability. Each agent was designed as a self-contained module responsible for a specific domain, with clearly defined inputs, outputs, and side effects. This modularity enables independent development, testing, and refinement of individual agents without requiring changes to the overall system architecture or other agents' implementations.

The design process for each agent began with clinical workflow analysis to understand the real-world processes being automated. For the appointment agent, this involved studying how scheduling staff handle patient calls, what information they collect, what validation steps they perform, and how they escalate urgent situations. For the medication agent, we analyzed pharmacist interactions and medication counseling sessions to identify common question patterns and critical safety checks. This grounding in actual healthcare workflows ensured that the automated system would align with established practices and clinical expectations.

Following workflow analysis, we conducted threat modeling to identify potential failure modes and safety risks. For each agent, we systematically considered questions such as: What happens if the system misclassifies intent? What if required data is missing? What if a patient provides conflicting information? What if the language model produces an inappropriate response? This threat modeling exercise informed the design of validation logic, fallback mechanisms, and safety guardrails that prevent the system from providing dangerous guidance or making inappropriate commitments.

### Routing Agent Implementation

The routing agent serves as the system's front door, responsible for determining which specialized agent should handle each incoming query and extracting key entities like patient identifiers. The implementation employs a two-tier approach that attempts large language model-based classification first, falling back to rule-based pattern matching if the model is unavailable or produces unparseable output. This hybrid approach balances the accuracy and flexibility of learned models with the reliability and interpretability of hand-crafted rules.

For language model-based classification, the routing agent constructs a prompt that presents the user's query along with descriptions of each available intent category. The prompt explicitly requests structured JSON output containing two fields: intent, which must be one of the predefined categories, and patient_id, which should contain any 8-digit number found in the input or null if none exists. The system message establishes that only valid JSON should be returned with no additional prose, minimizing the need for complex output parsing. When the model responds, the routing agent attempts to parse the JSON and validate that the intent field contains a recognized value.

The rule-based fallback mechanism applies keyword matching with carefully ordered precedence rules. The system first checks for emergency-related keywords like "chest pain", "can't breathe", or "bleeding" and maps these to the appointment intent since they require immediate scheduling attention. It then checks for appointment-related terms like "schedule", "reschedule", "cancel", or "appointment". Symptom-related keywords like "feel", "symptom", "dizzy", or "pain" map to followup intent. Medication keywords include "medication", "medicine", "side effect", "dose". Caregiver keywords include "summary" or "caregiver". If no keywords match, the query defaults to the help intent for general conversation handling.

Patient identifier extraction uses a regular expression pattern that matches exactly eight consecutive digits with word boundaries on either side. This pattern reliably identifies patient IDs in natural language queries like "I am patient 12345678" or "My ID is 12345678" while avoiding false matches on phone numbers or other numeric data. If multiple 8-digit numbers appear in the input, the system conservatively extracts the first occurrence and includes a warning in the logs for manual review. The extracted patient ID is validated against the database to ensure the patient record exists before proceeding with agent execution.

### Appointment Agent Architecture

The appointment agent represents the most complex component of the system, integrating scheduling logic, comprehensive triage capabilities, and policy enforcement within a unified workflow. The agent's processing pipeline begins with subintent classification to distinguish between status checking, new scheduling, rescheduling, and cancellation requests. This classification uses both language model inference and keyword matching, with the model trained to recognize subtle linguistic distinctions like the difference between "When is my appointment?" (status) and "When can I come in?" (schedule).

The core innovation of the appointment agent lies in its triage system, which implements clinical decision support logic to classify symptom urgency and route patients to appropriate care levels. The triage system operates on a three-tier model common in emergency medicine: RED flags indicating life-threatening emergencies requiring immediate intervention, ORANGE flags indicating concerning symptoms requiring same-day assessment, and GREEN indicating routine situations appropriate for standard scheduling. This classification directly impacts the agent's response, with RED-flagged symptoms triggering emergency department referrals and provider alerts rather than normal scheduling processes.

The triage logic is implemented through a rule-based system that evaluates symptom mentions against curated lists of clinical indicators. RED flag rules are defined for several categories of emergency symptoms. For chest pain, the system checks for pattern matches including "chest pain", "pain in my chest", "chest tightness", and "tightness in my chest" using case-insensitive substring matching. For respiratory distress, patterns include "short of breath", "can't breathe", "trouble breathing", and "shortness of breath". High fever is identified through numeric threshold comparison, flagging temperatures at or above 101.5 degrees Fahrenheit. Severe pain is detected through severity score thresholds, with scores of 8 or higher on the 0-10 scale triggering RED classification.

Additional RED flag categories cover wound complications, neurological deficits, and syncope. Wound-related patterns detect phrases like "wound opening", "drainage from wound", or "wound infection". Neurological indicators include "numbness", "tingling", "weakness in limb", and "slurred speech". Syncope patterns capture "fainted", "passed out", and "blacked out". Each rule is accompanied by clinical documentation explaining the rationale for its classification level, enabling healthcare providers reviewing the system's decisions to understand the underlying logic.

ORANGE flag rules follow similar pattern-matching logic but with lower severity thresholds. Moderate pain severity between 5 and 7 out of 10 triggers ORANGE classification, as does fever between 99.5 and 101.4 degrees Fahrenheit. Dizziness keywords like "dizzy" or "lightheaded" without other alarming symptoms result in ORANGE classification. For diabetic patients, blood glucose readings above 300 mg/dL are classified as ORANGE. Wound-related ORANGE flags include "redness around wound" or "swelling at surgical site" that suggest inflammation but not acute infection.

The policy enforcement component of the appointment agent implements business rules that govern scheduling eligibility and requirements. The minor consent policy checks patient age and, for patients under 18, verifies that a parent consent flag is set in the database before proceeding with scheduling. If consent is not documented, the agent politely declines to schedule and instructs the patient to have a parent or guardian contact the facility. The referral policy applies to specialist appointments, checking whether the patient has a referral on file before scheduling with certain specialties. Exceptions are made for primary care, which typically does not require referrals.

Telehealth eligibility validation checks multiple criteria including symptom severity, technical feasibility, and patient preference. Patients with RED flag symptoms are excluded from telehealth options regardless of preference, as these situations require in-person assessment. For ORANGE flags, telehealth eligibility is determined case-by-case based on symptom type, with some conditions amenable to video assessment and others requiring physical examination. The system also validates that patients have the technical capability and internet access required for video visits before proposing telehealth options.

Post-operative monitoring rules apply enhanced scrutiny to patients within 30 days of surgical procedures. The system automatically checks the patient's surgical history and calculates days since operation. For patients in this post-op window, any symptom reports are flagged for surgical team review regardless of triage tier, and appointment scheduling prioritizes slots with the patient's surgeon. This enhanced monitoring reflects the elevated risk of complications in the immediate post-surgical period and ensures that surgical teams maintain close oversight of their patients.

### Followup Agent Design

The followup agent specializes in logging patient-reported symptoms and providing immediate feedback on symptom severity. Unlike the appointment agent's focus on scheduling actions, the followup agent's primary responsibility is accurate documentation of symptom details in structured formats that enable trend analysis and clinical review. The agent's workflow begins with symptom extraction using natural language processing to identify symptom names, severity scores, body locations, temporal information, and contextual details from free-text patient reports.

Symptom extraction employs large language models with carefully structured prompts that request JSON output containing symptom arrays. Each symptom object includes fields for the symptom name in standardized terminology, severity on a 0-10 scale if mentioned, anatomical location if specified, and relevant context like triggering factors or associated symptoms. The prompt includes examples of proper extraction to guide the model toward consistent output formatting. For instance, the input "I feel dizzy maybe 7 out of 10 and my head hurts a bit" should produce two symptom objects: one for dizziness with severity 7 and one for headache with unspecified severity.

When language model extraction is unavailable, the agent falls back to keyword-based symptom detection using a curated dictionary of common symptom terms. This dictionary maps colloquial expressions to standardized symptom names, handling variations like "dizzy" and "dizziness" mapping to the same concept. The fallback mechanism also employs regular expressions to extract severity scores from patterns like "7/10", "seven out of ten", or "severity 7". While less sophisticated than model-based extraction, this fallback ensures the system continues functioning when API access is disrupted.

Each extracted symptom is written to a symptom log file that maintains temporal records of all patient reports. The log entry includes patient identifier, timestamp in ISO 8601 format, symptom name, severity score, fever temperature if mentioned, body location, and any additional notes. This structured logging enables several valuable downstream analyses. Clinicians can review symptom trajectories over time to assess treatment effectiveness. The caregiver agent aggregates these logs to identify concerning trends for weekly summaries. Quality improvement teams can analyze symptom report patterns to identify opportunities for proactive intervention.

After logging symptoms, the followup agent applies the same triage logic used by the appointment agent to classify urgency and provide appropriate guidance. RED flag symptoms trigger immediate recommendations to seek emergency care, with specific instructions about calling 911 or proceeding to the emergency department. The agent explicitly states that this is an urgent situation requiring immediate attention, using language designed to convey seriousness without causing panic. ORANGE flag symptoms result in promises of same-day nurse callback, setting expectations that clinical staff will review the report and contact the patient within hours. GREEN symptoms are logged with reassurance that the report will be reviewed by the provider at the next scheduled appointment.

### Medication Agent Implementation

The medication agent addresses patient questions about medications through a combination of knowledge base lookup and risk assessment. The agent's architecture recognizes six distinct intent categories that cover the majority of medication-related patient inquiries: side effect questions, missed dose scenarios, drug interaction concerns, administration instructions, contraindication warnings, and general medication information. Each intent category requires different information retrieval strategies and response templates to provide relevant, actionable guidance.

Intent classification for medication queries uses language model inference with prompts specifically tuned for pharmaceutical terminology. The prompt asks the model to identify which medication-related topic the patient is asking about and to extract any drug names mentioned in the query. Drug name extraction handles both brand names and generic names, with normalization logic that maps common variations to standard names. For example, "metformin" and "glucophage" are recognized as referring to the same medication. The extracted drug names are used to query the medication knowledge base and retrieve relevant information.

The medication knowledge base is implemented as a structured CSV file containing entries for commonly prescribed medications. Each entry includes the drug name, therapeutic class, common side effects, serious side effects requiring immediate attention, guidance for missed doses, food and timing recommendations, known drug interactions, and contraindication warnings. This information is sourced from authoritative references including FDA drug labels and clinical pharmacology databases. While the current knowledge base covers approximately 50 medications representing the most commonly prescribed drugs in outpatient settings, the architecture allows straightforward expansion to comprehensive coverage.

For side effect inquiries, the agent retrieves both common and serious side effects from the knowledge base and presents them in context. Common side effects are framed as expected, manageable effects that often improve with continued use, while serious side effects are explicitly labeled as requiring immediate medical attention. The agent balances being informative without being alarmist, acknowledging that side effects are possible while emphasizing that most patients tolerate medications well. The response includes guidance to contact the prescribing provider if side effects are bothersome or persistent.

Missed dose guidance follows pharmaceutical best practices that depend on timing relative to the next scheduled dose. The knowledge base includes medication-specific instructions that account for the drug's half-life and dosing frequency. For most medications, the guidance is to take the missed dose as soon as remembered unless it is close to the time for the next dose, in which case patients should skip the missed dose and resume the regular schedule. The agent explicitly warns against doubling doses to make up for missed ones, as this can lead to adverse effects or toxicity.

Drug interaction checking is one of the agent's most safety-critical functions. When patients ask about taking multiple medications together or adding new medications or supplements, the agent checks the knowledge base for documented interactions. Interactions are classified by severity as contraindicated (should never be combined), serious (requires close monitoring), or moderate (may require dose adjustment). The agent's response varies by interaction severity, with contraindicated combinations resulting in explicit warnings not to combine the medications without physician guidance. For all interaction concerns, the agent recommends consulting with the prescribing provider or pharmacist before making changes to medication regimens.

Risk assessment logic classifies each medication query by potential harm level using the same three-tier system employed for symptom triage. RED risk queries include double dosing scenarios, serious drug interaction concerns, and questions suggesting contraindicated medication use such as pregnancy-related inquiries for teratogenic drugs. These queries trigger responses that begin with explicit safety warnings and recommendations for immediate medical consultation. ORANGE risk queries include missed doses that may require clinical guidance, minor interaction concerns, and bothersome side effects that may need assessment. GREEN risk queries cover general informational questions without immediate safety implications.

### Caregiver Agent Capabilities

The caregiver agent generates weekly summaries of patient status for family members or other designated caregivers, supporting their role in monitoring and assisting with healthcare management. The agent's functionality is carefully designed to balance the value of caregiver engagement with patient privacy rights and regulatory requirements. Before generating any summary, the agent validates that the patient has both a designated caregiver on file and has provided explicit consent for information sharing with that caregiver.

Consent validation occurs through database queries that check for a caregiver relationship record and examine the patient's consent flags. If either the caregiver relationship or the sharing consent is absent, the agent declines the request and explains the privacy requirements. This consent-first approach ensures compliance with HIPAA privacy rules and respects patient autonomy regarding healthcare information disclosure. The agent maintains audit logs of all summary generation requests, including both approved and declined requests, to support privacy compliance monitoring.

For patients with proper consent, the summary generation process begins with data aggregation across multiple sources over a rolling seven-day window. The agent retrieves all symptom reports logged during this period, including symptom names, severity scores, and timestamps. Medication records are accessed to identify all active prescriptions and compare against symptom report frequency as a proxy for medication adherence. Appointment records are queried to identify any visits during the period and upcoming scheduled appointments. This aggregated view provides a comprehensive picture of the patient's healthcare activity and status.

Symptom trend analysis applies statistical methods to identify patterns in the aggregated symptom data. For symptoms reported multiple times during the week, the agent calculates average severity, identifies minimum and maximum scores, and determines whether severity is improving, stable, or worsening based on temporal trends. Worsening trends are flagged prominently in the summary as they may indicate treatment ineffectiveness or disease progression requiring clinical attention. The agent also identifies new symptoms that appeared during the reporting period but were not previously documented.

Medication adherence estimation uses a simplified model that compares expected versus actual reporting patterns. While true adherence monitoring would require integration with smart pill bottles or pharmacy fill data, the agent approximates adherence by examining whether symptom reports are occurring at expected frequency given the patient's chronic conditions. Unexpectedly absent symptom reports may suggest the patient is not consistently monitoring their symptoms or potentially not taking medications as prescribed. This rough proxy is supplemented with explicit notes indicating the limitations of the adherence estimate.

The narrative summary generation uses large language models to transform the structured aggregated data into readable prose suitable for caregiver consumption. The prompt instructs the model to write a concise summary that opens with patient identification, summarizes symptom trends using plain language, describes medication adherence status, lists upcoming appointments, and provides actionable recommendations for the caregiver. The generated summary is reviewed through automated checks that ensure it does not contain personally identifiable information beyond what is necessary, does not include overly technical medical terminology, and maintains an appropriate, supportive tone.

Generated summaries are stored in two formats to support different use cases. A JSON-formatted version maintains the structured data and is appended to a JSONL log file that enables programmatic analysis and trend tracking over multiple weeks. A plain text version formatted for readability is both logged and delivered to the caregiver, either through email, secure messaging, or printed for mailing depending on the caregiver's preferences. This dual format approach supports both human consumption and automated analytics.

---

## Validation

### Evaluation Methodology

The validation of the Voice Agent system employed a comprehensive testing strategy that combined quantitative accuracy metrics, qualitative response assessment, and client-facing validation datasets. The testing approach recognized that different aspects of system performance require different evaluation methods. Intent classification and triage decisions lend themselves to quantitative metrics with ground truth labels, while response quality and conversational naturalness require human judgment. Safety-critical components like emergency triage were held to higher standards than general conversation quality.

The core of the quantitative evaluation consisted of systematically generated test cases that exercise all major system capabilities across diverse scenarios. Rather than manually crafting expected outputs, the validation approach involved executing test cases through the actual system workflow and capturing real responses generated by the large language models. This approach provides authentic examples of system behavior under realistic conditions, though it required careful design of test inputs with clear expected classifications to enable accuracy assessment.

Test case design followed a structured methodology that ensured comprehensive coverage of system capabilities. For each agent, we identified key scenarios representing common use cases, edge cases that might challenge the system's logic, and safety-critical situations that must be handled correctly to avoid patient harm. The appointment agent test suite included routine appointment checks, scheduling requests, emergency presentations requiring RED triage, concerning symptoms requiring ORANGE classification, policy violation scenarios, and edge cases like patients without appointments or ambiguous scheduling requests.

Similarly, medication test cases covered all six intent categories with multiple examples per category. Side effect inquiries ranged from common, expected effects to serious reactions requiring medical attention. Missed dose scenarios included morning doses taken at noon, evening doses taken at bedtime, and doses completely skipped. Drug interaction checks tested combinations of patient medications, addition of over-the-counter supplements, and contraindicated combinations. This systematic coverage ensured that evaluation results would generalize beyond a handful of cherry-picked examples.

### Intent Classification Performance

Intent classification accuracy was evaluated across 122 test cases split between appointment-related queries (64 cases) and medication-related queries (58 cases). Each test case included a natural language input and a ground truth label indicating the correct intent classification. The inputs were designed to reflect realistic patient language patterns, including colloquialisms, incomplete sentences, and expressions that could plausibly be ambiguous. The system's routing agent classified each input, and results were compared against ground truth labels to compute accuracy metrics.

Overall intent classification achieved 98.4% accuracy, correctly routing 120 of 122 test cases to the appropriate specialized agent. The two misclassified cases both involved medication queries with unusual phrasing that the model interpreted as general help requests rather than medication-specific inquiries. Detailed error analysis revealed that these misclassifications occurred when queries lacked explicit medication-related keywords and instead used indirect references like "the pills I take in the morning" without specifying drug names or mentioning medication directly. This finding suggests that improved prompt engineering or additional training examples for edge cases could further improve classification accuracy.

Breaking down performance by agent category, appointment intent classification achieved perfect 100% accuracy across all 64 test cases. This high performance reflects the presence of clear distinguishing keywords in appointment-related queries such as "schedule", "reschedule", "cancel", and "appointment". The routing agent's hybrid approach of using both language model inference and keyword fallback provides robust classification even when queries are phrased unconventionally. For example, "I need to come in soon" was correctly classified as a scheduling request despite not containing the word "appointment", demonstrating the model's ability to understand semantic intent.

Medication intent classification reached 96.6% accuracy with 56 of 58 correct classifications. The sub-intent classification within medication queries, distinguishing between side effects, missed doses, interactions, instructions, contraindications, and general information, achieved 93.1% accuracy. Most errors involved confusion between similar intent categories, such as classifying a contraindication question as a general information request. These confusions have minimal practical impact since the medication agent provides relevant information regardless of fine-grained intent classification, but they do suggest opportunities for improved intent taxonomies or more explicit prompting.

### Triage System Evaluation

Triage classification performance was assessed with particular attention to emergency sensitivity, as failure to detect life-threatening symptoms represents the most serious potential safety failure of the system. The test suite included 31 emergency scenarios with RED flag symptoms, 15 concerning scenarios with ORANGE flag symptoms, and 18 routine scenarios with GREEN classification. Each scenario included explicit symptom descriptions designed to trigger specific triage rules, and the system's classifications were compared against clinical ground truth.

RED flag sensitivity, the most critical metric, reached 96.8% with 30 of 31 emergency scenarios correctly identified. The single false negative represents a serious safety concern that requires immediate remediation. Analysis of this failure revealed a pattern matching limitation where the patient's description of "tightness in my chest" was semantically similar to but not an exact string match for the triage rule pattern "tightness in my chest". The large language model's symptom extraction produced a slightly different phrasing that did not trigger the rule, resulting in the symptom being documented but not classified as an emergency.

This false negative highlights a fundamental limitation of rule-based triage systems that rely on exact pattern matching. While the current implementation uses substring matching that should accommodate minor variations, the interaction between language model extraction and rule evaluation introduces potential failure modes. The symptom extraction model may paraphrase patient descriptions into clinically equivalent but lexically different terms that fail to match rule patterns. This finding has driven a planned enhancement to implement semantic similarity matching using vector embeddings, where symptom descriptions would be compared to rule patterns in embedding space rather than string space.

ORANGE flag classification achieved 100% accuracy across all 15 test cases, correctly identifying concerning symptoms that warrant same-day clinical assessment but not immediate emergency intervention. These scenarios included moderate pain severities (5-7 out of 10), low-grade fevers (99.5-101.4Â°F), isolated dizziness, hyperglycemia over 300 mg/dL, and wound inflammation signs. The perfect performance on ORANGE classification demonstrates that the triage logic effectively distinguishes between urgent and emergent scenarios, avoiding both excessive emergency escalation and dangerous under-triage.

GREEN classification also achieved 100% accuracy across 18 routine scenarios. These cases included appointment status checks without symptom mentions, general medication questions without concerning side effects, and mild symptoms with low severity scores. The correct classification of GREEN cases is important for system usability, as excessive false alarms would undermine clinical trust and potentially desensitize staff to system alerts. The high specificity for routine scenarios indicates that the system appropriately reserves escalation for situations that genuinely warrant clinical attention.

Beyond binary classification accuracy, we evaluated the triage system's rule explainability by examining which specific rules triggered for each case. The system maintains a list of matched rule names for every classification, enabling post-hoc review of triage reasoning. This analysis revealed that most RED classifications were triggered by single, unambiguous rules such as the chest pain pattern or severe pain threshold. A few cases triggered multiple rules simultaneously, such as a patient reporting both high fever and severe pain, which provides redundant confirmation of emergency status. This redundancy is clinically desirable as it reduces the chance that rule updates or modifications would inadvertently create dangerous gaps in emergency detection.

### Medication Risk Assessment

The medication agent's risk assessment capabilities were evaluated across all 58 medication test cases, with each case labeled for expected risk tier (RED, ORANGE, or GREEN) based on the nature of the query and potential for patient harm. The risk assessment logic combines the classified intent (what type of medication question) with the specific content (which medications, what situation) to determine appropriate risk level and response urgency.

RED risk detection achieved perfect 100% accuracy across 8 high-risk scenarios that included double dosing questions, serious drug interaction concerns, and contraindicated medication use. The double dosing scenarios explicitly mentioned taking two doses of medications like blood pressure medications or anticoagulants, which the system correctly identified as requiring immediate medical attention. Drug interaction scenarios involved combinations known to have serious consequences, such as taking anticoagulants with NSAIDs or combining medications that prolong QT interval. The system's responses to these RED risk queries appropriately began with explicit safety warnings and urged immediate consultation with healthcare providers.

ORANGE risk classification was correct in 93.3% of cases, with 14 of 15 scenarios correctly identified. These scenarios primarily involved missed dose questions where timing and medication type made clinical guidance advisable. For example, a patient asking about a missed morning dose of insulin was correctly classified as ORANGE due to the critical nature of diabetes management and the need for personalized guidance. The single misclassified case involved a vague question about medication timing that the system interpreted as general information (GREEN) rather than a potential adherence issue (ORANGE). While this misclassification resulted in a less urgent response, the provided information was still relevant and appropriate.

GREEN risk queries, representing routine informational questions without safety concerns, were correctly classified in 100% of 35 cases. These included questions about medication purposes, general side effect information presented as proactive learning rather than current symptoms, and administration guidance for medications being taken as prescribed. The consistent accuracy for GREEN classification indicates that the system appropriately reserves higher risk tiers for situations with actual or potential adverse outcomes, avoiding unnecessary alarm for standard medication education requests.

Beyond classification accuracy, we evaluated the appropriateness and completeness of the medication agent's responses through manual review. For side effect inquiries, responses consistently included both common and serious side effects with appropriate framing. For missed dose questions, guidance matched published recommendations from drug manufacturers and clinical guidelines. Drug interaction warnings correctly identified contraindicated combinations and appropriately recommended provider consultation for potential interactions requiring monitoring.

### Policy Enforcement Validation

Policy enforcement logic was tested through targeted scenarios designed to trigger each business rule. The test suite included 12 policy-relevant cases: 3 minor consent scenarios (patients under 18), 3 specialist referral scenarios, 3 telehealth eligibility scenarios, and 3 post-operative monitoring scenarios. For each policy rule, we constructed cases that should be blocked or flagged by the policy, along with control cases that should pass policy checks.

Minor consent policy enforcement achieved 100% accuracy, correctly identifying all three scenarios where patients under 18 years old attempted to schedule appointments without documented parental consent. The system's responses to these scenarios appropriately declined to complete scheduling and instructed the minor patient to have a parent or guardian contact the facility. The responses used age-appropriate language that explained the requirement without being dismissive or creating barriers to eventual care access.

Specialist referral validation was correct in all three test cases, blocking scheduling attempts for specialty appointments when the patient lacked a documented referral. The system's responses explained the referral requirement and provided guidance for obtaining referrals from primary care providers. Notably, the logic correctly implemented exceptions for certain specialties like primary care and routine eye examinations that typically do not require referrals, demonstrating nuanced understanding of referral policies rather than blanket rules.

Telehealth eligibility decisions were appropriate in all three scenarios, which tested different aspects of eligibility criteria. A RED triage case was correctly excluded from telehealth options with explanation that in-person examination was necessary. An established patient with GREEN status requesting telehealth was approved. A new patient requesting telehealth was directed to initial in-person visit based on the policy requiring established care relationships for virtual visits. These correct decisions across different eligibility dimensions indicate robust policy logic.

Post-operative monitoring flags were correctly applied in all three scenarios involving patients within 30 days of surgery. The system appropriately flagged symptom reports from these patients for surgical team review and prioritized scheduling with the patient's surgeon. This enhanced monitoring applies regardless of symptom triage tier, reflecting the elevated risk profile of recent surgical patients. The responses clearly communicated to patients that their post-operative status would ensure appropriate surgical oversight.

### Response Quality Assessment

While quantitative metrics assess the correctness of classifications and decisions, the ultimate user experience depends heavily on response quality dimensions that require human judgment. We conducted manual review of 50 randomly sampled agent responses across all agent types, assessing each response on four dimensions using 5-point Likert scales: clarity (is the response easy to understand?), empathy (does it acknowledge patient concerns appropriately?), completeness (does it fully address the query?), and actionability (does it provide clear next steps?).

The average overall quality score across all dimensions was 4.3 out of 5.0, indicating generally high-quality responses that would be acceptable in clinical communication. Breaking down by dimension, clarity scored highest at 4.5, reflecting the system's consistent use of plain language and logical response structure. Completeness scored 4.4, showing that responses generally addressed all aspects of patient queries. Empathy scored 4.2, demonstrating appropriate acknowledgment of patient concerns though with room for improvement in warmth. Actionability scored 4.1, the lowest dimension, suggesting that some responses could more explicitly state concrete next steps.

Examining score distributions, 86% of responses received ratings of 4 or 5 (good or excellent) across all dimensions. Only 6% received ratings below 3 (poor quality requiring improvement), and these were concentrated in specific failure modes. The most common quality issues involved verbosity, where responses included excessive detail or repeated information unnecessarily. Some responses used medical terminology like "syncope" or "contraindication" without defining terms, potentially confusing patients with limited health literacy. A few responses for ambiguous queries asked clarifying questions but failed to provide any provisional guidance, leaving patients without actionable information.

The highest-quality responses shared several characteristics. They opened with explicit acknowledgment of the patient's situation, such as "I understand you're concerned about chest pain." They used clear signposting language to structure multi-part responses. They consistently ended with specific next steps stated in concrete terms like "A nurse will call you within 2 hours" rather than vague promises of follow-up. They balanced medical accuracy with plain language, explaining clinical terms when necessary and avoiding jargon when possible.

Comparing response quality across agent types revealed some variation. The appointment agent's responses scored highest (4.5 average) due to the well-structured nature of scheduling interactions and clear action items. The medication agent scored 4.3, performing well on informational content but sometimes producing overly long responses when addressing multiple medications. The followup agent scored 4.2, with some responses being too brief and not sufficiently acknowledging the patient's symptom reports. The help agent scored 4.1, reflecting the inherent difficulty of handling diverse, open-ended queries without clear workflows.

### Validation Dataset Generation

A key component of the validation strategy involved generating comprehensive test datasets that could be shared with clinical stakeholders for external review. Rather than creating synthetic expected outputs manually, we developed an automated validation dataset generator that executes test cases through the actual system workflow and captures real responses. This approach ensures that the validation data reflects authentic system behavior under realistic conditions, providing stakeholders with genuine examples of how the system would respond to patient queries.

The validation dataset generator implements a structured process that begins with curated test case definitions. Each test case specifies the user input query, the patient ID if relevant, the scenario category (e.g., "Emergency Triage - RED Flag"), and the expected intent and triage classifications. The generator executes each test case by constructing an appropriate initial state, invoking the LangGraph workflow, and capturing the resulting state after agent execution. From the result state, the generator extracts relevant fields including detected intent, assigned triage tier, matched triage rules, policy enforcement results, symptom details, and the complete agent response.

For the appointment agent, we generated 64 test cases spanning routine appointment operations, emergency triage scenarios across all RED flag categories, ORANGE flag presentations, policy enforcement situations, and edge cases like missing patient records or ambiguous requests. The resulting validation dataset contains 16 columns of information per test case, providing rich detail that enables multiple types of analysis. Stakeholders can examine whether triage tiers match clinical expectations, whether policy enforcement aligns with organizational requirements, whether responses are appropriately phrased, and whether the system correctly handles edge cases.

The medication agent validation dataset contains 58 test cases covering all six intent categories with multiple examples per category representing different medication classes and clinical situations. Each test case captures the classified intent, assigned risk tier, the complete agent response, parsed drug mentions, and detected language. This structure enables reviewers to assess both classification accuracy and response appropriateness across diverse medication scenarios.

The validation datasets are exported to CSV format for easy review in spreadsheet applications that clinical stakeholders commonly use. The CSV format also facilitates programmatic analysis, enabling calculation of accuracy metrics, identification of systematic errors, and trend analysis across scenario categories. Documentation accompanying the datasets explains the column meanings, describes the test case generation methodology, and provides guidance for interpreting results and providing feedback.

Early sharing of these validation datasets with project stakeholders has proven valuable for iterative improvement. Clinical reviewers identified the chest tightness pattern matching issue described earlier, which would have been difficult to discover through manual testing due to the subtle nature of the language model's paraphrasing. Stakeholders also suggested additional edge cases to include in future test expansions, such as scenarios involving language barriers, cognitive impairments affecting symptom reporting, or unusual medication regimens. This external validation process continues to drive system refinement and ensures alignment with clinical expectations.

---

## Expansion and Future Work

### Planned Short-Term Enhancements

Several high-priority improvements have been identified through the validation process and stakeholder feedback, representing achievable enhancements that would significantly improve system safety and utility. The most critical enhancement addresses the pattern matching limitation discovered in triage evaluation. Rather than relying on exact string matching, the improved system will implement semantic similarity matching using vector embeddings. Symptom descriptions from patients and triage rule patterns will both be encoded into embedding vectors using models like sentence transformers. Similarity scoring will identify when patient descriptions are semantically equivalent to flagged symptoms even when phrasing differs. This approach should eliminate the false negative observed while maintaining the interpretability and clinical reviewability that make rule-based systems preferable to opaque learned models.

Medication knowledge base expansion represents another high-impact enhancement with straightforward implementation. The current coverage of approximately 50 medications can be systematically expanded to 500+ drugs representing comprehensive coverage of outpatient pharmaceuticals. This expansion will proceed through structured data collection from authoritative sources including FDA drug labels, clinical pharmacology references, and drug interaction databases. Standardized templates will ensure consistent data quality across entries. Priority will be given to medications commonly prescribed for chronic conditions managed in outpatient settings, followed by acute treatment medications and then specialty pharmaceuticals.

Response quality improvements will focus on several dimensions identified through manual review. Prompt engineering refinements will encourage more concise responses that convey necessary information without excessive detail. Response templates for common scenarios will ensure consistent quality and include best-practice phrasing developed through stakeholder feedback. Tone adjustment logic will modulate response urgency and empathy based on the triage tier, with emergency responses conveying appropriate urgency while routine interactions maintain supportive but efficient tone. Medical jargon reduction will be achieved through automated text analysis that identifies technical terms and flags them for replacement with plain language alternatives or explicit definitions.

Enhanced logging and analytics capabilities will provide the monitoring infrastructure necessary for production deployment. Conversation turn tracking will maintain complete interaction histories that enable analysis of multi-turn dialog patterns and identification of conversational breakdowns. Patient satisfaction scoring can be implemented through optional feedback prompts after interactions, providing direct measurement of user experience. Response time metrics will be instrumented throughout the processing pipeline to identify performance bottlenecks and enable optimization efforts. A clinical oversight dashboard will aggregate key metrics including daily triage tier distributions, policy enforcement actions, error rates, and flagged interactions requiring manual review.

### Medium-Term Development Goals

Electronic health record integration represents a transformative capability that would transition the system from demonstration prototype to production-ready clinical tool. The current CSV-based database implementation successfully demonstrates all logical capabilities but imposes practical limitations for real-world deployment. EHR integration would enable real-time patient data retrieval, ensuring that the system operates on current information including recent appointments, updated medication lists, and new diagnoses. The integration approach will leverage FHIR (Fast Healthcare Interoperability Resources) APIs that provide standardized interfaces to major EHR systems like Epic and Cerner.

The architectural design already anticipates EHR integration through the database service abstraction layer. The current CSV implementation will be replaced with a FHIR client that translates the database service's method calls into appropriate FHIR queries. For example, the get_patient method currently reads from patients.csv but will be reimplemented to execute a FHIR Patient resource query. This approach preserves all higher-level agent logic while swapping the underlying data source. Bidirectional synchronization will ensure that information collected by the Voice Agent, particularly symptom logs, flows back into the EHR for clinical review and care coordination.

Advanced triage capabilities will leverage the structured data accumulated through system usage to implement more sophisticated clinical decision support. Multi-symptom correlation analysis will identify combinations of symptoms that collectively indicate serious conditions even when individual symptoms are mild. For example, the combination of mild dyspnea, mild chest discomfort, and fatigue in a patient with cardiac history might warrant elevated concern compared to any single symptom alone. Risk stratification will incorporate patient history including previous diagnoses, recent hospitalizations, active conditions, and documented risk factors to personalize triage decisions. Integration with clinical decision support frameworks will enable the system to apply evidence-based guidelines and clinical pathways.

A dedicated caregiver portal will enhance the existing summary generation capabilities by providing interactive access to patient information. Rather than receiving weekly summaries passively, caregivers could log into a web interface to view real-time updates, access historical trends, and configure notification preferences. Push notifications would alert caregivers to concerning developments like symptom worsening or missed appointments. Medication reminder functionality could send notifications to both patients and caregivers about upcoming doses. Secure messaging would enable caregivers to communicate questions to the clinical team through the portal, with responses visible to authorized family members.

Voice interface enhancements will improve the accessibility and naturalness of spoken interactions. Speech-to-text accuracy can be improved by fine-tuning on medical vocabulary and common patient descriptions of symptoms. Current off-the-shelf STT models occasionally misrecognize medical terms or medication names, requiring context-aware correction. Text-to-speech naturalness will benefit from updated synthesis models that incorporate emotional prosody, varying pace and tone based on content urgency and emotional context. Multi-language support will expand access to non-English speaking patients, beginning with Spanish given its prevalence in many healthcare markets. Accent and dialect adaptation will ensure accurate recognition across diverse patient populations.

### Long-Term Vision

The long-term vision for the Voice Agent system extends beyond reactive patient support to proactive, predictive healthcare management powered by machine learning and advanced analytics. Predictive models trained on historical patient data could identify elevated readmission risk based on symptom trajectories, medication adherence patterns, appointment attendance, and clinical factors. Rather than waiting for patients to report concerning developments, the system would proactively reach out to high-risk individuals with targeted interventions. These interventions might include enhanced monitoring, accelerated follow-up scheduling, medication adherence support, or care coordination referrals.

Trend-based early warning systems will continuously analyze incoming symptom reports to detect subtle patterns that precede clinical deterioration. For example, gradual increases in pain severity over several days, progressive dyspnea despite stable baseline disease, or new symptoms appearing in clusters might trigger alerts even when individual reports remain below threshold. Machine learning models can learn these patterns from historical data, identifying sequences that frequently precede readmissions or emergency visits. This predictive capability transforms the system from a reactive documentation tool into a proactive guardian that intervenes before situations become acute.

Clinical workflow integration will embed the Voice Agent into providers' existing systems and processes rather than operating as a standalone tool. Automated triage queue population will surface high-priority patient interactions directly in clinicians' work lists, prioritized by urgency and clinical significance. A physician dashboard would present escalated cases with relevant context including triage reasoning, patient history, and recommended next steps. Integration with on-call scheduling systems would route urgent issues to available providers while respecting coverage arrangements. Closed-loop communication tracking would monitor whether flagged interactions resulted in clinical follow-up, identifying cases that require additional attention.

Multi-modal interaction capabilities will expand beyond text and voice to incorporate visual and sensor data. Image upload functionality would allow patients to photograph wounds, skin conditions, or medication labels for clinical review. Computer vision models could provide automated initial assessment, flagging concerning features like spreading erythema or dehiscence. Video consultation scheduling and hosting would seamlessly transition from Voice Agent interactions to synchronous video visits when indicated. Integration with wearable devices would ingest continuous data streams including activity levels, heart rate patterns, sleep quality, and fall detection. Vital sign trending from connected devices like blood pressure monitors or pulse oximeters would enable quantitative tracking of clinical parameters.

Research and quality improvement applications will leverage the rich data generated by Voice Agent interactions to advance clinical knowledge and optimize operations. A de-identified data repository containing interaction transcripts, triage decisions, and clinical outcomes would support health services research into topics like symptom recognition, clinical decision-making, and healthcare communication. A/B testing frameworks would enable rigorous evaluation of system modifications, comparing alternative triage rules, response templates, or intervention strategies to identify approaches that optimize patient outcomes. Outcome tracking linking Voice Agent interactions to subsequent healthcare utilization would enable measurement of the system's impact on readmissions, emergency visits, and care costs. Cost-effectiveness analyses would quantify the return on investment by comparing implementation costs against reductions in avoidable acute care utilization.

### Integration with Knowledge Graph System

The Voice Agent and Knowledge Graph systems developed as parallel components of this capstone project have significant potential for synergistic integration. The Knowledge Graph focuses on encoding medical coverage policies, prior authorization requirements, and clinical guidelines into structured, queryable representations. The Voice Agent provides conversational interfaces for patient interaction and care management. Combining these capabilities would create a unified system that grounds patient interactions in policy logic while maintaining accessible, natural language interfaces.

One powerful integration scenario involves policy-driven appointment scheduling where coverage and authorization requirements dynamically influence scheduling decisions. When a patient requests a specialist appointment, the Voice Agent would query the Knowledge Graph to determine whether the visit requires prior authorization based on the patient's insurance plan, the requested specialty, and clinical indications. The Voice Agent's response would incorporate this policy information, potentially initiating authorization workflows or informing patients of coverage limitations before appointments are scheduled. This integration would reduce downstream authorization denials, improve revenue cycle efficiency, and enhance patient satisfaction by setting appropriate expectations upfront.

Clinical pathway adherence represents another valuable integration point where the Knowledge Graph defines evidence-based care pathways for specific conditions and the Voice Agent guides patients through pathway steps. For example, a post-surgical pathway might specify required follow-up appointments, symptom monitoring frequency, activity restrictions, and physical therapy schedules. The Voice Agent could reference this pathway when interacting with recently discharged surgical patients, proactively prompting for expected appointments, inquiring about pathway-specified symptoms, and reinforcing adherence to activity restrictions. Deviations from the pathway would be flagged for clinical review, enabling care coordination interventions.

Compliance monitoring applications would use the Knowledge Graph to encode regulatory requirements and quality metrics while the Voice Agent's interaction logs provide data for compliance assessment. For instance, regulations requiring diabetes education or depression screening for specific patient populations could be represented in the Knowledge Graph. The Voice Agent's logs would document when such education or screening occurred, enabling automated compliance reporting and identification of care gaps. This approach reduces manual chart review burden while ensuring comprehensive compliance documentation.

The technical implementation of these integrations would leverage API-based communication between the two systems. The Voice Agent would expose endpoints for querying patient interaction history, current monitoring status, and flagged clinical concerns. The Knowledge Graph would provide query interfaces for policy lookups, pathway retrieval, and compliance checks. Shared data models using FHIR or similar standards would ensure semantic interoperability. Event-driven architectures using message queues could enable reactive integrations where policy updates automatically trigger Voice Agent notification logic or patient outreach campaigns.

---

**This document is under active development as part of the CMU x Zyter Capstone Project.**
